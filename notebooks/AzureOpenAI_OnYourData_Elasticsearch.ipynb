{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f84108f",
   "metadata": {},
   "source": [
    "# Microsoft Azure OpenAI On Your Data with Elasticsearch\n",
    "\n",
    "In this notebook we'll use Elasticsearch indices to provide grounding data for queries to Azure OpenAI models using the Azure OpenAI On Your Data service.\n",
    "\n",
    "The Azure OpenAI On Your Data service currently supports three search scenarios for retrieval of documents that will be sent to the LLM for processing:\n",
    "\n",
    "1) full text search\n",
    "2) vector search using Elasticsearch Machine Learning models\n",
    "3) vector search using embeddings generated using Azure OpenAI (Ada).\n",
    "\n",
    "Each of these examples will be covered in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cd732",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "For this example, you will need:\n",
    "* Python 3.6 or later\n",
    "* An Elastic deployment meeting the following criteria: with machine learning node\n",
    "    * API version 8.x\n",
    "    * A machine learning node for following the example for vector search using an Elasticsearch text embedding model\n",
    "* An Azure OpenAI Resource\n",
    "    * At minimum, one chat model should be deployed for your resource to enable chatting about your data.\n",
    "    * Optionally, if you would like to try out vector search using the Azure OpenAI Ada model, you will also need to deploy an Ada model to your resource.  The examples below will assume you are using the model `text-embedding-ada-002`, but can be updated to suit your needs.\n",
    "* The [Elastic Python client](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/installation.html)\n",
    "* The [OpenAI Python Client](https://platform.openai.com/docs/api-reference/introduction?lang=python)\n",
    "\n",
    "### Create Elastic Deployment\n",
    "\n",
    "If you don't have an Elastic deployment, you can read more about how to get started here in the official [\"Getting Started\" guide for Elastic Cloud](https://www.elastic.co/getting-started).\n",
    "\n",
    "\n",
    "### Configure Azure OpenAI Resource\n",
    "\n",
    "If you don't have an Azure OpenAI resource, detailed information about how to obtain one can be found in the [official documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/use-your-data-quickstart?tabs=command-line&pivots=programming-language-python) for the Azure OpenAI On Your Data service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfff496a",
   "metadata": {},
   "source": [
    "## Install packages and initialize environment\n",
    "\n",
    "The first step is to use `pip` to install all of the packages we need to connect to Elasticsearch and Azure OpenAI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "65760c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU elasticsearch openai==0.28.1 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a5580b",
   "metadata": {},
   "source": [
    "Here we will also define a few helper classes and functions that will be reused throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f940364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "from io import StringIO\n",
    "from time import sleep, perf_counter\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "class SampleCsvDatasets(object):\n",
    "    datasets = {\n",
    "        \"msmarco\": {\n",
    "            \"url\": \"https://github.com/mayya-sharipova/msmarco/raw/main/msmarco-passagetest2019-unique.tsv\",\n",
    "            \"fieldnames\": [\"id\", \"text\"],\n",
    "            \"delimiter\": \"\\t\"\n",
    "        }\n",
    "    }\n",
    "    def __init__(self):\n",
    "        self._values = {}\n",
    "        for name, config in self.datasets.items():\n",
    "            url = config.get(\"url\")\n",
    "            response = urlopen(url)\n",
    "            self._values[name] = self._create_dataframe(name, response.read().decode(\"utf-8\"))\n",
    "    \n",
    "    def _create_dataframe(self, dataset_name, value):\n",
    "        return pd.read_csv(\n",
    "            StringIO(value),\n",
    "            delimiter=self.datasets.get(dataset_name).get(\"delimiter\"),\n",
    "            names=self.datasets.get(dataset_name).get(\"fieldnames\")\n",
    "        )\n",
    "    \n",
    "    def get_dataframe(self, dataset_name, add_aoai_embeddings=False):\n",
    "        return self._values[name]\n",
    "    \n",
    "    def get_dataframe_with_aoai_embeddings(\n",
    "        self,\n",
    "        dataset_name,\n",
    "        embedding_model_deployment_name,\n",
    "        text_column_name=\"text\",\n",
    "        embeddings_column_name=\"text_embedding.aoai_predicted_value\",\n",
    "        model_name=\"text-embedding-ada-002\"\n",
    "    ):\n",
    "        df = self._values[dataset_name]\n",
    "        \n",
    "        # 16 is the current maximum batch size for embeddings, so we'll partition the dataframe by 16 rows.\n",
    "        maximum_batch_size = 16\n",
    "        \n",
    "        # This value can be manipulated if rate limiting errors occur while producing embeddings.\n",
    "        throttling_interval = 0.5\n",
    "        \n",
    "        # Occasionally the server will become overloaded, so retrying can ensure success.\n",
    "        retry_count = 30\n",
    "        \n",
    "        number_of_buckets = math.ceil(len(df) / maximum_batch_size)\n",
    "        partitions = np.array_split(df, number_of_buckets)\n",
    "        embeddings = []\n",
    "        request_times = []\n",
    "        \n",
    "        start = perf_counter()\n",
    "        for partition in partitions:\n",
    "            success = False\n",
    "            for i in range(retry_count):\n",
    "                try:\n",
    "                    request_start = perf_counter()\n",
    "                    text_embedding_response = openai.Embedding.create(\n",
    "                        deployment_id=embedding_model_deployment_name,\n",
    "                        input=partition[text_column_name].tolist(),\n",
    "                        api_version=\"2023-03-15-preview\"\n",
    "                    )\n",
    "                    embeddings.extend([result[\"embedding\"] for result in text_embedding_response[\"data\"]])\n",
    "                    sleep(throttling_interval)\n",
    "                    request_end = perf_counter()\n",
    "                    request_times.append(request_end-request_start)\n",
    "                    success = True\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Encountered the following exception when creating embeddings (retrying): {str(e)}\")\n",
    "                    sleep(5)\n",
    "            \n",
    "            if not success:\n",
    "                raise Exception(\"Unable to generate embeddings (see previous error output for more information.)\")\n",
    "        \n",
    "        end = perf_counter()\n",
    "        df[text_embeddings_column_name] == embeddings\n",
    "        print(f\"Average request time for {len(partition)} records: {statistics.mean(request_times)}\")\n",
    "        print(f\"Total time: {end-start}\")\n",
    "\n",
    "def create_index(\n",
    "    elastic_client,\n",
    "    index_name: str,\n",
    "    **properties\n",
    "):\n",
    "    mappings = {\n",
    "        \"properties\": properties\n",
    "    }\n",
    "\n",
    "    # Clean up any previously created index with the same name\n",
    "    elastic_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "\n",
    "    # Create the index\n",
    "    elastic_client.indices.create(index=index_name, mappings=mappings)\n",
    "\n",
    "\n",
    "def index_data(\n",
    "    elastic_client,\n",
    "    dataset_reader,\n",
    "    index_name\n",
    "):\n",
    "    # Use the bulk API to index data in chunks of 10000 documents\n",
    "    operations_chunks = []\n",
    "    chunk = []\n",
    "    max_operations_chunk_size = 10000\n",
    "    chunk_size = 0\n",
    "\n",
    "    for row in dataset_reader:\n",
    "\n",
    "        if chunk_size < max_operations_chunk_size:\n",
    "            chunk.append({\"index\": {\"_index\": index_name}})\n",
    "            chunk.append(row)\n",
    "            chunk_size += 1\n",
    "            \n",
    "        else:\n",
    "            operations_chunks.append(chunk)\n",
    "            chunk = []\n",
    "            chunk_size = 0\n",
    "\n",
    "    if len(chunk) > 0:\n",
    "        operations_chunks.append(chunk)\n",
    "\n",
    "    for chunk in operations_chunks:\n",
    "        elastic_client.bulk(index=index_name, operations=chunk, refresh=True)\n",
    "\n",
    "def chat_with_my_data(\n",
    "    chat_query,\n",
    "    aoai_deployment_name,\n",
    "    elasticsearch_endpoint,\n",
    "    elasticsearch_api_key,\n",
    "    index_name,\n",
    "    **embedding_config\n",
    "):\n",
    "    elasticsearch_embedding_model = embedding_config.get(\"elasticsearch_embedding_model\", None)\n",
    "    aoai_embedding_model = embedding_config.get(\"aoai_embedding_model\", None)\n",
    "    aoai_embedding_key = embedding_config.get(\"aoai_embedding_key\", None)\n",
    "    aoai_embedding_endpoint = None if not aoai_embedding_model and not aoai_embedding_key \\\n",
    "        else  f\"{openapi.api_base}/openai/deployments/{aoai_embedding_model}/embeddings?api-version=2023-03-15-preview\"\n",
    "    \n",
    "    query_type = \"vector\" if (aoai_embedding_endpoint and aoai_embedding_key) or elasticsearch_embedding_model else \"simple\"\n",
    "    \n",
    "    completion = openai.ChatCompletion.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": chat_query\n",
    "            }\n",
    "        ],\n",
    "        dataSources=[\n",
    "            {\n",
    "                \"type\": \"Elasticsearch\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": elasticsearch_endpoint,\n",
    "                    \"encodedApiKey\": elasticsearch_api_key,\n",
    "                    \"indexName\": index_name,\n",
    "                    \"queryType\": query_type,\n",
    "                    \"embeddingModelId\": elasticsearch_embedding_model,\n",
    "                    \"embeddingEndpoint\":  aoai_embedding_endpoint,\n",
    "                    \"embeddingKey\": aoai_embedding_key\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        deployment_id=aoai_deployment_name\n",
    "    )\n",
    "    print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2fdccc",
   "metadata": {},
   "source": [
    "Next, let's set some variables for configuring access to the services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "36f42661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import openai\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Elasticsearch Configuration\n",
    "elasticsearch_endpoint = input(\"Elasticsearch endpoint: \")\n",
    "elasticsearch_api_key = getpass.getpass(\"Elasticsearch API Key: \")\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "openai.api_base = input(\"Azure OpenAI resource endpoint: \")\n",
    "openai.api_key = getpass.getpass(\"Azure OpenAI resource key: \")\n",
    "chat_model_deployment_name = input(\"Azure OpenAI chat model deployment name: \")\n",
    "embedding_model_deployment_name = input(\"Azure OpenAI embedding model deployment name (enter 'None' if not applicable): \" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879fadd",
   "metadata": {},
   "source": [
    "We'll also initialize our dataset manager for easy data access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2baf659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = SampleCsvDatasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04781fb",
   "metadata": {},
   "source": [
    "Next, we'll configure the Elasticsearch client, which we will use to index some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4050818b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'instance-0000000001', 'cluster_name': '211e3ff0aa8e41ab8d8fc9bde647e87d', 'cluster_uuid': 'ySrbugeMRISJ2YQQ_ZR8gA', 'version': {'number': '8.10.1', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'a94744f97522b2b7ee8b5dc13be7ee11082b8d6b', 'build_date': '2023-09-14T20:16:27.027355296Z', 'build_snapshot': False, 'lucene_version': '9.7.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "elastic_client = Elasticsearch(\n",
    "    f\"{elasticsearch_endpoint}:443\",\n",
    "    api_key=elasticsearch_api_key\n",
    ")\n",
    "\n",
    "print(elastic_client.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ea191",
   "metadata": {},
   "source": [
    "Next, we'll configure the OpenAI client to make requests to our specific Azure OpenAI resource endpoint.  We need to create an instance of `requests.adapters.HTTPAdapter` to ensure that the request session URL used by the OpenAI client points to the correct Azure OpenAI resource URL for each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13a59211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import Session\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "class OnYourDataAdapter(HTTPAdapter):\n",
    "    def send(self, request, **kwargs):\n",
    "        request.url = f\"{openai.api_base}/openai/deployments/{chat_model_deployment_name}/extensions/chat/completions?api-version={openai.api_version}\"\n",
    "        return super().send(request, **kwargs)\n",
    "    \n",
    "\n",
    "session = Session()\n",
    "session.mount(\n",
    "    prefix=f\"{openai.api_base}/openai/deployments/{chat_model_deployment_name}\",\n",
    "    adapter=OnYourDataAdapter()\n",
    ")\n",
    "\n",
    "openai.requestssession = session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315211dc",
   "metadata": {},
   "source": [
    "## Example 1: Grounding ChatGPT with data retrieved from a full-text search query \n",
    "\n",
    "\n",
    "### Create Elasticsearch index with required mappings\n",
    "\n",
    "We need to create an index which contains some text data for using full-text search for retrieving documents to ground the responses from Azure OpenAI.  First, we'll create the index mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "902e3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticsearch_index_name = \"msmarco-passagetest2019-unique\"\n",
    "mapping_properties = {\n",
    "    \"id\": {\"type\": \"keyword\"},\n",
    "    \"text\": {\"type\": \"text\"}\n",
    "}\n",
    "\n",
    "create_index(\n",
    "    elastic_client,\n",
    "    elasticsearch_index_name,\n",
    "    **mapping_properties\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e040e",
   "metadata": {},
   "source": [
    "#### Download dataset\n",
    "\n",
    "For this example, we'll a subset of the MS MARCO Passage Ranking dataset.  We'll use the dataset manager to get a dataframe of our data to use for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ee9cab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = datasets.get_dataframe(\"msmarco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67981bbd",
   "metadata": {},
   "source": [
    "#### Index documents\n",
    "\n",
    "We'll call the helper function `index_data` to handle the indexing.  This function uses the `bulk` API to index data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f8eaa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_data(\n",
    "    elastic_client,\n",
    "    dataframe,\n",
    "    elasticsearch_index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a072892",
   "metadata": {},
   "source": [
    "### Chat about your dataset\n",
    "\n",
    "Now that we have some data available in our Elasticsearch cluster, we can use Azure OpenAI On Your Data to ask questions about it.  The `chat_with_my_data` function call below will use full-text search by default, since we are not passing in any additional configuration about embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e34f9221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the retrieved documents, Jamaica has perfect weather most of the year, and the best time to visit Jamaica depends on personal preference. The weather in December is described as cool and it is considered high season[doc1][doc2][doc3][doc5]. Additionally, January and February are also considered high season[doc3][doc5]. For more detailed information on monthly weather averages, including temperature, rainfall, and sunshine, the Jamaica climate guides are recommended for holiday planning[doc4].\n"
     ]
    }
   ],
   "source": [
    "chat_query = \"How's the weather in Jamaica?\"\n",
    "chat_with_my_data(\n",
    "    chat_query,\n",
    "    chat_model_deployment_name,\n",
    "    elasticsearch_endpoint,\n",
    "    elasticsearch_api_key,\n",
    "    elasticsearch_index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf209c6",
   "metadata": {},
   "source": [
    "## Example 2: Grounding ChatGPT with data retrieved from a kNN search query using an Elasticsearch machine learning model for embeddings\n",
    "\n",
    "The next example will show how to chat with your data using a vector search query using embeddings produced by a model deployed to your Elasticsearch cluster.  This example is adapted from the Elastic Search Labs guide [\"How to deploy NLP: Text Embeddings and Vector Search\"](https://www.elastic.co/search-labs/how-to-deploy-nlp-text-embeddings-and-vector-search), and uses the same dataset that we already downloaded in the previous example.\n",
    "\n",
    "First, we will need to install the `eland` package, which we will use to deploy an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02fdf05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting eland[pytorch]\n",
      "  Downloading eland-8.10.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: elasticsearch<9,>=8.3 in c:\\users\\abhahn\\appdata\\local\\miniconda3\\envs\\jupyter\\lib\\site-packages (from eland[pytorch]) (8.10.1)\n",
      "Collecting pandas<2,>=1.5 (from eland[pytorch])\n",
      "  Downloading pandas-1.5.3-cp310-cp310-win_amd64.whl (10.4 MB)\n",
      "     ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/10.4 MB 653.6 kB/s eta 0:00:16\n",
      "      --------------------------------------- 0.2/10.4 MB 1.7 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 0.6/10.4 MB 4.1 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 1.7/10.4 MB 8.9 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 4.3/10.4 MB 18.3 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 6.4/10.4 MB 22.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 7.2/10.4 MB 23.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 8.2/10.4 MB 21.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 9.9/10.4 MB 24.3 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.4/10.4 MB 29.8 MB/s eta 0:00:00\n",
      "Collecting matplotlib>=3.6 (from eland[pytorch])\n",
      "  Downloading matplotlib-3.8.1-cp310-cp310-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting numpy<1.24,>=1.2.0 (from eland[pytorch])\n",
      "  Downloading numpy-1.23.5-cp310-cp310-win_amd64.whl (14.6 MB)\n",
      "     ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 2.7/14.6 MB 56.6 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 3.5/14.6 MB 57.0 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 6.6/14.6 MB 52.3 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 8.1/14.6 MB 47.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 9.4/14.6 MB 42.8 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 10.4/14.6 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 12.7/14.6 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.6/14.6 MB 46.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 14.6/14.6 MB 40.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\abhahn\\appdata\\local\\miniconda3\\envs\\jupyter\\lib\\site-packages (from eland[pytorch]) (23.1)\n",
      "Collecting torch<2.0,>=1.13.1 (from eland[pytorch])\n",
      "  Downloading torch-1.13.1-cp310-cp310-win_amd64.whl (162.6 MB)\n",
      "     ---------------------------------------- 0.0/162.6 MB ? eta -:--:--\n",
      "      -------------------------------------- 2.7/162.6 MB 56.4 MB/s eta 0:00:03\n",
      "      -------------------------------------- 3.7/162.6 MB 59.8 MB/s eta 0:00:03\n",
      "     - ------------------------------------- 7.8/162.6 MB 56.0 MB/s eta 0:00:03\n",
      "     -- ------------------------------------ 8.8/162.6 MB 47.1 MB/s eta 0:00:04\n",
      "     -- ------------------------------------ 9.8/162.6 MB 41.7 MB/s eta 0:00:04\n",
      "     -- ----------------------------------- 12.3/162.6 MB 40.9 MB/s eta 0:00:04\n",
      "     --- ---------------------------------- 15.2/162.6 MB 50.4 MB/s eta 0:00:03\n",
      "     ---- --------------------------------- 18.1/162.6 MB 46.7 MB/s eta 0:00:04\n",
      "     ---- --------------------------------- 21.1/162.6 MB 59.5 MB/s eta 0:00:03\n",
      "     ----- -------------------------------- 24.2/162.6 MB 54.7 MB/s eta 0:00:03\n",
      "     ----- -------------------------------- 24.7/162.6 MB 59.5 MB/s eta 0:00:03\n",
      "     ------ ------------------------------- 26.8/162.6 MB 50.4 MB/s eta 0:00:03\n",
      "     ------ ------------------------------- 29.0/162.6 MB 46.9 MB/s eta 0:00:03\n",
      "     ------- ------------------------------ 31.8/162.6 MB 50.4 MB/s eta 0:00:03\n",
      "     -------- ----------------------------- 34.4/162.6 MB 50.4 MB/s eta 0:00:03\n",
      "     -------- ----------------------------- 36.6/162.6 MB 54.7 MB/s eta 0:00:03\n",
      "     --------- ---------------------------- 38.6/162.6 MB 59.5 MB/s eta 0:00:03\n",
      "     --------- ---------------------------- 41.3/162.6 MB 59.5 MB/s eta 0:00:03\n",
      "     ---------- --------------------------- 43.6/162.6 MB 59.8 MB/s eta 0:00:02\n",
      "     ---------- --------------------------- 46.4/162.6 MB 59.5 MB/s eta 0:00:02\n",
      "     ----------- -------------------------- 49.1/162.6 MB 59.5 MB/s eta 0:00:02\n",
      "     ----------- -------------------------- 51.3/162.6 MB 59.5 MB/s eta 0:00:02\n",
      "     ------------ ------------------------- 53.5/162.6 MB 59.5 MB/s eta 0:00:02\n",
      "     ------------- ------------------------ 56.5/162.6 MB 59.5 MB/s eta 0:00:02\n",
      "     ------------- ------------------------ 59.2/162.6 MB 59.5 MB/s eta 0:00:02\n",
      "     -------------- ----------------------- 61.3/162.6 MB 59.5 MB/s eta 0:00:02\n",
      "     -------------- ----------------------- 64.1/162.6 MB 59.5 MB/s eta 0:00:02\n",
      "     --------------- ---------------------- 66.6/162.6 MB 59.5 MB/s eta 0:00:02\n",
      "     ---------------- --------------------- 69.3/162.6 MB 59.5 MB/s eta 0:00:02\n",
      "     ---------------- --------------------- 71.6/162.6 MB 54.4 MB/s eta 0:00:02\n",
      "     ----------------- -------------------- 73.8/162.6 MB 54.4 MB/s eta 0:00:02\n",
      "     ----------------- -------------------- 75.8/162.6 MB 50.1 MB/s eta 0:00:02\n",
      "     ------------------ ------------------- 77.3/162.6 MB 46.9 MB/s eta 0:00:02\n",
      "     ------------------ ------------------- 79.3/162.6 MB 43.7 MB/s eta 0:00:02\n",
      "     ------------------- ------------------ 82.0/162.6 MB 54.7 MB/s eta 0:00:02\n",
      "     ------------------- ------------------ 84.6/162.6 MB 43.7 MB/s eta 0:00:02\n",
      "     -------------------- ----------------- 87.1/162.6 MB 50.4 MB/s eta 0:00:02\n",
      "     -------------------- ----------------- 89.7/162.6 MB 54.4 MB/s eta 0:00:02\n",
      "     --------------------- ---------------- 92.4/162.6 MB 54.4 MB/s eta 0:00:02\n",
      "     ---------------------- --------------- 94.7/162.6 MB 54.4 MB/s eta 0:00:02\n",
      "     ---------------------- --------------- 97.2/162.6 MB 54.7 MB/s eta 0:00:02\n",
      "     ----------------------- -------------- 99.9/162.6 MB 54.7 MB/s eta 0:00:02\n",
      "     ----------------------- ------------- 102.4/162.6 MB 54.7 MB/s eta 0:00:02\n",
      "     ----------------------- ------------- 105.2/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------ ------------ 107.8/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------- ----------- 110.7/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------- ----------- 113.5/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     -------------------------- ---------- 116.2/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     --------------------------- --------- 119.1/162.6 MB 59.8 MB/s eta 0:00:01\n",
      "     --------------------------- --------- 122.0/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ---------------------------- -------- 124.7/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ---------------------------- -------- 127.3/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ----------------------------- ------- 130.0/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ----------------------------- ------- 131.8/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------ ------ 134.4/162.6 MB 50.4 MB/s eta 0:00:01\n",
      "     ------------------------------- ----- 137.2/162.6 MB 50.1 MB/s eta 0:00:01\n",
      "     ------------------------------- ----- 139.8/162.6 MB 50.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 142.6/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 145.2/162.6 MB 54.7 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 147.7/162.6 MB 59.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 150.6/162.6 MB 65.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 153.6/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 156.6/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  158.9/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  161.5/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  162.6/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  162.6/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  162.6/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  162.6/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------  162.6/162.6 MB 59.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- 162.6/162.6 MB 27.3 MB/s eta 0:00:00\n",
      "Collecting sentence-transformers<=2.2.2,>=2.1.0 (from eland[pytorch])\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "     ---------------------------------------- 0.0/86.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 86.0/86.0 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers<=4.33.2,>=4.31.0 (from transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch])\n",
      "  Downloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\n",
      "     ---------------------------------------- 0.0/119.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 119.9/119.9 kB 6.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: elastic-transport<9,>=8 in c:\\users\\abhahn\\appdata\\local\\miniconda3\\envs\\jupyter\\lib\\site-packages (from elasticsearch<9,>=8.3->eland[pytorch]) (8.10.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.6->eland[pytorch])\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.6->eland[pytorch])\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.6->eland[pytorch])\n",
      "  Downloading fonttools-4.44.0-cp310-cp310-win_amd64.whl.metadata (156 kB)\n",
      "     ---------------------------------------- 0.0/156.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 156.8/156.8 kB ? eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.6->eland[pytorch])\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting pillow>=8 (from matplotlib>=3.6->eland[pytorch])\n",
      "  Downloading Pillow-10.1.0-cp310-cp310-win_amd64.whl.metadata (9.6 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=3.6->eland[pytorch])\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\abhahn\\appdata\\local\\miniconda3\\envs\\jupyter\\lib\\site-packages (from matplotlib>=3.6->eland[pytorch]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\abhahn\\appdata\\local\\miniconda3\\envs\\jupyter\\lib\\site-packages (from pandas<2,>=1.5->eland[pytorch]) (2023.3.post1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\abhahn\\appdata\\local\\miniconda3\\envs\\jupyter\\lib\\site-packages (from sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch]) (4.66.1)\n",
      "Collecting torchvision (from sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch])\n",
      "  Downloading torchvision-0.16.0-cp310-cp310-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting scikit-learn (from sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch])\n",
      "  Downloading scikit_learn-1.3.2-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch])\n",
      "  Downloading scipy-1.11.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.4/60.4 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting nltk (from sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch])\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.5/1.5 MB 48.4 MB/s eta 0:00:00\n",
      "Collecting sentencepiece (from sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch])\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-win_amd64.whl (977 kB)\n",
      "     ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 977.5/977.5 kB 60.5 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.4.0 (from sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch])\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\abhahn\\appdata\\local\\miniconda3\\envs\\jupyter\\lib\\site-packages (from torch<2.0,>=1.13.1->eland[pytorch]) (4.7.1)\n",
      "Collecting filelock (from transformers<=4.33.2,>=4.31.0->transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch])\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\abhahn\\appdata\\local\\miniconda3\\envs\\jupyter\\lib\\site-packages (from transformers<=4.33.2,>=4.31.0->transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch]) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers<=4.33.2,>=4.31.0->transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch])\n",
      "  Using cached regex-2023.10.3-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\abhahn\\appdata\\local\\miniconda3\\envs\\jupyter\\lib\\site-packages (from transformers<=4.33.2,>=4.31.0->transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch]) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<=4.33.2,>=4.31.0->transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch])\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "     ------------------------------- -------- 2.7/3.5 MB 87.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.5/3.5 MB 55.8 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1 (from transformers<=4.33.2,>=4.31.0->transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch])\n",
      "  Downloading safetensors-0.4.0-cp310-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting accelerate>=0.20.3 (from transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch])\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\abhahn\\appdata\\local\\miniconda3\\envs\\jupyter\\lib\\site-packages (from accelerate>=0.20.3->transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch]) (5.9.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\users\\abhahn\\appdata\\local\\miniconda3\\envs\\jupyter\\lib\\site-packages (from elastic-transport<9,>=8->elasticsearch<9,>=8.3->eland[pytorch]) (1.26.18)\n",
      "Requirement already satisfied: certifi in c:\\users\\abhahn\\appdata\\local\\miniconda3\\envs\\jupyter\\lib\\site-packages (from elastic-transport<9,>=8->elasticsearch<9,>=8.3->eland[pytorch]) (2023.7.22)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.4.0->sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch])\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abhahn\\appdata\\local\\miniconda3\\envs\\jupyter\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.6->eland[pytorch]) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhahn\\appdata\\local\\miniconda3\\envs\\jupyter\\lib\\site-packages (from tqdm->sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch]) (0.4.6)\n",
      "Collecting click (from nltk->sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch])\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk->sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch])\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhahn\\appdata\\local\\miniconda3\\envs\\jupyter\\lib\\site-packages (from requests->transformers<=4.33.2,>=4.31.0->transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhahn\\appdata\\local\\miniconda3\\envs\\jupyter\\lib\\site-packages (from requests->transformers<=4.33.2,>=4.31.0->transformers[torch]<=4.33.2,>=4.31.0; extra == \"pytorch\"->eland[pytorch]) (3.4)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch])\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from sentence-transformers<=2.2.2,>=2.1.0->eland[pytorch])\n",
      "  Downloading torchvision-0.15.2-cp310-cp310-win_amd64.whl (1.2 MB)\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.2/1.2 MB 38.2 MB/s eta 0:00:00\n",
      "  Downloading torchvision-0.15.1-cp310-cp310-win_amd64.whl (1.2 MB)\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.2/1.2 MB 74.1 MB/s eta 0:00:00\n",
      "  Downloading torchvision-0.14.1-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.1/1.1 MB 72.2 MB/s eta 0:00:00\n",
      "Downloading matplotlib-3.8.1-cp310-cp310-win_amd64.whl (7.6 MB)\n",
      "   ---------------------------------------- 0.0/7.6 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 3.1/7.6 MB 65.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.9/7.6 MB 62.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.6 MB 61.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.6/7.6 MB 54.5 MB/s eta 0:00:00\n",
      "Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
      "   ---------------------------------------- 0.0/7.6 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 2.3/7.6 MB 48.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.2/7.6 MB 54.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.6 MB 54.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.6/7.6 MB 48.9 MB/s eta 0:00:00\n",
      "Downloading eland-8.10.1-py3-none-any.whl (156 kB)\n",
      "   ---------------------------------------- 0.0/156.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 156.4/156.4 kB 9.1 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
      "   ---------------------------------------- 0.0/261.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 261.4/261.4 kB 15.7 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.2.0-cp310-cp310-win_amd64.whl (186 kB)\n",
      "   ---------------------------------------- 0.0/186.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 186.7/186.7 kB 11.0 MB/s eta 0:00:00\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.44.0-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.1/2.1 MB 68.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 45.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/302.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 302.0/302.0 kB 18.2 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.5-cp310-cp310-win_amd64.whl (56 kB)\n",
      "   ---------------------------------------- 0.0/56.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 56.1/56.1 kB ? eta 0:00:00\n",
      "Downloading Pillow-10.1.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.6/2.6 MB 83.8 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "   ---------------------------------------- 0.0/103.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 103.1/103.1 kB ? eta 0:00:00\n",
      "Using cached regex-2023.10.3-cp310-cp310-win_amd64.whl (269 kB)\n",
      "Downloading safetensors-0.4.0-cp310-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 277.4/277.4 kB ? eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading scikit_learn-1.3.2-cp310-cp310-win_amd64.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 3.2/9.3 MB 103.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.2/9.3 MB 78.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.9/9.3 MB 63.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 59.3 MB/s eta 0:00:00\n",
      "Downloading scipy-1.11.3-cp310-cp310-win_amd64.whl (44.1 MB)\n",
      "   ---------------------------------------- 0.0/44.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 2.2/44.1 MB 143.4 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 5.2/44.1 MB 65.9 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 7.9/44.1 MB 63.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 10.3/44.1 MB 65.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 13.1/44.1 MB 59.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 15.9/44.1 MB 59.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 18.0/44.1 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 20.9/44.1 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 23.6/44.1 MB 59.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 26.3/44.1 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 28.0/44.1 MB 50.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 30.8/44.1 MB 54.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 33.7/44.1 MB 54.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 35.8/44.1 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 38.5/44.1 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.5/44.1 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.4/44.1 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 59.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.1/44.1 MB 46.9 MB/s eta 0:00:00\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 166.4/166.4 kB 10.4 MB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 302.2/302.2 kB 19.5 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125953 sha256=2786e6820753e3a67eec15ec4d61ea97e09614081f6b88c98d25367ded4fed0c\n",
      "  Stored in directory: c:\\users\\abhahn\\appdata\\local\\pip\\cache\\wheels\\62\\f2\\10\\1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, torch, threadpoolctl, safetensors, regex, pyparsing, pillow, numpy, kiwisolver, joblib, fsspec, fonttools, filelock, cycler, click, torchvision, scipy, pandas, nltk, huggingface-hub, contourpy, transformers, scikit-learn, matplotlib, accelerate, sentence-transformers, eland\n",
      "Successfully installed accelerate-0.24.1 click-8.1.7 contourpy-1.2.0 cycler-0.12.1 eland-8.10.1 filelock-3.13.1 fonttools-4.44.0 fsspec-2023.10.0 huggingface-hub-0.18.0 joblib-1.3.2 kiwisolver-1.4.5 matplotlib-3.8.1 nltk-3.8.1 numpy-1.23.5 pandas-1.5.3 pillow-10.1.0 pyparsing-3.1.1 regex-2023.10.3 safetensors-0.4.0 scikit-learn-1.3.2 scipy-1.11.3 sentence-transformers-2.2.2 sentencepiece-0.1.99 threadpoolctl-3.2.0 tokenizers-0.13.3 torch-1.13.1 torchvision-0.14.1 transformers-4.33.2\n"
     ]
    }
   ],
   "source": [
    "!pip install eland[pytorch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c7581",
   "metadata": {},
   "source": [
    "Next, we will deploy the model `msmarco-MiniLM-L-12-v3` model from Hugging Face, which is trained on a superset of the data we have already indexed in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b31d2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:37:35,452 INFO : Establishing connection to Elasticsearch\n",
      "2023-11-06 18:37:36,151 INFO : Connected to cluster named '211e3ff0aa8e41ab8d8fc9bde647e87d' (version: 8.10.1)\n",
      "2023-11-06 18:37:36,154 INFO : Loading HuggingFace transformer tokenizer and model 'sentence-transformers/msmarco-MiniLM-L-12-v3'\n",
      "\n",
      "(…)12-v3/resolve/main/tokenizer_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]\n",
      "(…)12-v3/resolve/main/tokenizer_config.json: 100%|##########| 432/432 [00:00<00:00, 419kB/s]\n",
      "C:\\Users\\abhahn\\AppData\\Local\\miniconda3\\envs\\jupyter\\lib\\site-packages\\huggingface_hub\\file_download.py:138: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhahn\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "\n",
      "(…)-MiniLM-L-12-v3/resolve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]\n",
      "(…)-MiniLM-L-12-v3/resolve/main/config.json: 100%|##########| 629/629 [00:00<?, ?B/s] \n",
      "\n",
      "(…)co-MiniLM-L-12-v3/resolve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\n",
      "(…)co-MiniLM-L-12-v3/resolve/main/vocab.txt: 100%|##########| 232k/232k [00:00<00:00, 3.94MB/s]\n",
      "\n",
      "(…)-v3/resolve/main/special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]\n",
      "(…)-v3/resolve/main/special_tokens_map.json: 100%|##########| 112/112 [00:00<?, ?B/s] \n",
      "\n",
      "pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]\n",
      "pytorch_model.bin:   8%|7         | 10.5M/134M [00:00<00:03, 32.3MB/s]\n",
      "pytorch_model.bin:  16%|#5        | 21.0M/134M [00:00<00:03, 37.3MB/s]\n",
      "pytorch_model.bin:  24%|##3       | 31.5M/134M [00:00<00:02, 38.3MB/s]\n",
      "pytorch_model.bin:  31%|###1      | 41.9M/134M [00:01<00:02, 44.4MB/s]\n",
      "pytorch_model.bin:  39%|###9      | 52.4M/134M [00:01<00:01, 43.4MB/s]\n",
      "pytorch_model.bin:  47%|####7     | 62.9M/134M [00:01<00:01, 45.2MB/s]\n",
      "pytorch_model.bin:  55%|#####4    | 73.4M/134M [00:01<00:01, 43.0MB/s]\n",
      "pytorch_model.bin:  63%|######2   | 83.9M/134M [00:02<00:01, 39.7MB/s]\n",
      "pytorch_model.bin:  71%|#######   | 94.4M/134M [00:02<00:00, 39.3MB/s]\n",
      "pytorch_model.bin:  79%|#######8  | 105M/134M [00:02<00:00, 38.5MB/s] \n",
      "pytorch_model.bin:  86%|########6 | 115M/134M [00:02<00:00, 37.4MB/s]\n",
      "pytorch_model.bin:  94%|#########4| 126M/134M [00:03<00:00, 37.1MB/s]\n",
      "pytorch_model.bin: 100%|##########| 134M/134M [00:03<00:00, 36.1MB/s]\n",
      "pytorch_model.bin: 100%|##########| 134M/134M [00:03<00:00, 38.8MB/s]\n",
      "\n",
      "(…)ed1625216e5bd1a2f517abaf1/.gitattributes:   0%|          | 0.00/736 [00:00<?, ?B/s]\n",
      "(…)ed1625216e5bd1a2f517abaf1/.gitattributes: 100%|##########| 736/736 [00:00<?, ?B/s] \n",
      "\n",
      "(…)16e5bd1a2f517abaf1/1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]\n",
      "(…)16e5bd1a2f517abaf1/1_Pooling/config.json: 100%|##########| 190/190 [00:00<?, ?B/s] \n",
      "\n",
      "(…)574bbed1625216e5bd1a2f517abaf1/README.md:   0%|          | 0.00/3.69k [00:00<?, ?B/s]\n",
      "(…)574bbed1625216e5bd1a2f517abaf1/README.md: 100%|##########| 3.69k/3.69k [00:00<?, ?B/s]\n",
      "\n",
      "(…)4bbed1625216e5bd1a2f517abaf1/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]\n",
      "(…)4bbed1625216e5bd1a2f517abaf1/config.json: 100%|##########| 629/629 [00:00<?, ?B/s] \n",
      "\n",
      "(…)7abaf1/config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]\n",
      "(…)7abaf1/config_sentence_transformers.json: 100%|##########| 122/122 [00:00<?, ?B/s] \n",
      "\n",
      "pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]\n",
      "pytorch_model.bin:   8%|7         | 10.5M/134M [00:00<00:03, 35.8MB/s]\n",
      "pytorch_model.bin:  16%|#5        | 21.0M/134M [00:00<00:03, 34.9MB/s]\n",
      "pytorch_model.bin:  24%|##3       | 31.5M/134M [00:00<00:02, 38.4MB/s]\n",
      "pytorch_model.bin:  31%|###1      | 41.9M/134M [00:01<00:02, 40.7MB/s]\n",
      "pytorch_model.bin:  39%|###9      | 52.4M/134M [00:01<00:01, 43.3MB/s]\n",
      "pytorch_model.bin:  47%|####7     | 62.9M/134M [00:01<00:01, 45.7MB/s]\n",
      "pytorch_model.bin:  55%|#####4    | 73.4M/134M [00:01<00:01, 43.7MB/s]\n",
      "pytorch_model.bin:  63%|######2   | 83.9M/134M [00:02<00:01, 40.4MB/s]\n",
      "pytorch_model.bin:  71%|#######   | 94.4M/134M [00:02<00:00, 41.3MB/s]\n",
      "pytorch_model.bin:  79%|#######8  | 105M/134M [00:02<00:00, 40.6MB/s] \n",
      "pytorch_model.bin:  86%|########6 | 115M/134M [00:02<00:00, 40.6MB/s]\n",
      "pytorch_model.bin:  94%|#########4| 126M/134M [00:03<00:00, 40.4MB/s]\n",
      "pytorch_model.bin: 100%|##########| 134M/134M [00:03<00:00, 40.9MB/s]\n",
      "pytorch_model.bin: 100%|##########| 134M/134M [00:03<00:00, 40.9MB/s]\n",
      "\n",
      "(…)bd1a2f517abaf1/sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]\n",
      "(…)bd1a2f517abaf1/sentence_bert_config.json: 100%|##########| 53.0/53.0 [00:00<?, ?B/s]\n",
      "\n",
      "(…)e5bd1a2f517abaf1/special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]\n",
      "(…)e5bd1a2f517abaf1/special_tokens_map.json: 100%|##########| 112/112 [00:00<?, ?B/s] \n",
      "\n",
      "(…)ed1625216e5bd1a2f517abaf1/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]\n",
      "(…)ed1625216e5bd1a2f517abaf1/tokenizer.json: 100%|##########| 466k/466k [00:00<00:00, 8.41MB/s]\n",
      "\n",
      "(…)16e5bd1a2f517abaf1/tokenizer_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]\n",
      "(…)16e5bd1a2f517abaf1/tokenizer_config.json: 100%|##########| 432/432 [00:00<?, ?B/s] \n",
      "\n",
      "(…)574bbed1625216e5bd1a2f517abaf1/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]\n",
      "(…)574bbed1625216e5bd1a2f517abaf1/vocab.txt: 100%|##########| 232k/232k [00:00<00:00, 31.8MB/s]\n",
      "\n",
      "(…)bbed1625216e5bd1a2f517abaf1/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]\n",
      "(…)bbed1625216e5bd1a2f517abaf1/modules.json: 100%|##########| 229/229 [00:00<?, ?B/s] \n",
      "2023-11-06 18:37:53,062 ERROR : Trained model with id 'sentence-transformers__msmarco-minilm-l-12-v3' already exists\n",
      "2023-11-06 18:37:53,062 INFO : Run the script with the '--clear-previous' flag if you want to overwrite the existing model.\n"
     ]
    }
   ],
   "source": [
    "!eland_import_hub_model \\\n",
    "    --url {elasticsearch_endpoint}:443 \\\n",
    "    --es-api-key {elasticsearch_api_key} \\\n",
    "    --hub-model-id sentence-transformers/msmarco-MiniLM-L-12-v3 \\\n",
    "    --task-type text_embedding \\\n",
    "    --start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570de4ea",
   "metadata": {},
   "source": [
    "Next, we can reindex our data with the embeddings from the model to enable kNN search using those embeddings.  In the next cell, we'll create an ingest pipeline to calculate embeddings from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "49109c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch.client import IngestClient\n",
    "\n",
    "pipeline_id = \"msmarco-minilm-l-12-v3\"\n",
    "ingest_processors = [\n",
    "    {\n",
    "      \"inference\": {\n",
    "        \"model_id\": \"sentence-transformers__msmarco-minilm-l-12-v3\",\n",
    "        \"target_field\": \"text_embedding\",\n",
    "        \"field_map\": {\n",
    "          \"text\": \"text_field\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "]\n",
    "\n",
    "on_failure = [\n",
    "    {\n",
    "      \"set\": {\n",
    "        \"description\": \"Index document to 'failed-<index>'\",\n",
    "        \"field\": \"_index\",\n",
    "        \"value\": \"failed-{{{_index}}}\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"set\": {\n",
    "        \"description\": \"Set error message\",\n",
    "        \"field\": \"ingest.failure\",\n",
    "        \"value\": \"{{_ingest.on_failure_message}}}\"\n",
    "      }\n",
    "    }\n",
    "]\n",
    "\n",
    "ingest_client = IngestClient(elastic_client)\n",
    "\n",
    "ingest_client.put_pipeline(\n",
    "    id=pipeline_id,\n",
    "    on_failure=on_failure,\n",
    "    processors=ingest_processors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da95e8",
   "metadata": {},
   "source": [
    "Then, we'll reindex the `msmarco-passagetest2019-unique` index using the pipeline we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "140fc17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'v2M-MYd8QDWzbhuNMsO24g:67500998'}\n"
     ]
    }
   ],
   "source": [
    "# Create the new index\n",
    "elasticsearch_embeddings_index_name = f\"msmarco-passagetest2019-unique-{pipeline_id}\"\n",
    "mapping_properties = {\n",
    "    \"text_embedding.predicted_value\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 384,\n",
    "        \"index\": True,\n",
    "        \"similarity\": \"cosine\"\n",
    "      },\n",
    "      \"text\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "}\n",
    "\n",
    "create_index(\n",
    "    elastic_client,\n",
    "    elasticsearch_embeddings_index_name,\n",
    "    **mapping_properties)\n",
    "\n",
    "# Reindex source data into target index\n",
    "reindex_source = {\n",
    "    \"index\": elasticsearch_index_name\n",
    "}\n",
    "reindex_dest = {\n",
    "    \"index\": elasticsearch_embeddings_index_name,\n",
    "    \"pipeline\": pipeline_id\n",
    "}\n",
    "\n",
    "response = elastic_client.reindex(\n",
    "    source=reindex_source,\n",
    "    dest=reindex_dest,\n",
    "    wait_for_completion=False\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3900f7",
   "metadata": {},
   "source": [
    "The reindexing operation may take some time, so we can poll for the task status before moving to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1c5bf795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reindexing is complete!\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "is_completed = False\n",
    "while not is_completed:\n",
    "    sleep(1)\n",
    "    task = elastic_client.tasks.get(task_id=response.get(\"task\"))\n",
    "    is_completed = task.get(\"completed\")\n",
    "    \n",
    "print(\"Reindexing is complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d6e4a",
   "metadata": {},
   "source": [
    "Now that we have our embeddings index ready to go, we can use this new index with embeddings with the Azure OpenAI On Your Data service.  Note that this call to `chat_with_my_data` passes in some configuration for our embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "818e393c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jamaica has a tropical and humid climate with warm to hot temperatures all year round, averaging between 80 and 90 degrees Fahrenheit. The days are warm and the nights are cooler, with mountain areas being cooler than the lower land throughout the year. Rain usually falls for short periods in the late afternoon, with sunshine the rest of the day[doc1][doc3][doc4][doc5]. The best time to visit Jamaica is year-round due to its consistently warm tropical weather, with peak season running from mid-December to mid-April when crowds swell and prices rise[doc2].\n"
     ]
    }
   ],
   "source": [
    "chat_query = \"What's the weather like in Jamaica?\"\n",
    "\n",
    "chat_with_my_data(\n",
    "    chat_query,\n",
    "    chat_model_deployment_name,\n",
    "    elasticsearch_endpoint,\n",
    "    elasticsearch_api_key,\n",
    "    elasticsearch_embeddings_index_name,\n",
    "    elasticsearch_embedding_model=\"sentence-transformers__msmarco-minilm-l-12-v3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2125fbee",
   "metadata": {},
   "source": [
    "## Example 3: Grounding ChatGPT with data retrieved from a kNN search query using Azure OpenAI embeddings\n",
    "\n",
    "We also have the option to create embeddings to use for kNN search using Azure OpenAI.  \n",
    "\n",
    "For this example, in addition to the chat model deployment used in the previous two examples for chatting with your data, you will also need an embedding model deployment added to your Azure OpenAI resource.  We will first use this embedding model to generate embeddings on your data, and then later refer to this deployment in our requests to the Azure OpenAI On Your Data service, which will call the model to generate the embeddings for your query as a part of the chat request.\n",
    "\n",
    "First, we will create a new index with the correct mappings for embeddings produced using the Azure OpenAI Ada embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df5e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_embeddings_index_name = f\"msmarco-passagetest2019-unique-{embedding_model_deployment_name}\"\n",
    "mapping_properties = {\n",
    "    \"id\": {\"type\": \"keyword\"},\n",
    "    \"text\": {\"type\": \"text\"},\n",
    "    \"text_embedding.aoai_predicted_value\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 1536,\n",
    "        \"index\": True,\n",
    "        \"similarity\": \"cosine\"\n",
    "    }\n",
    "}\n",
    "\n",
    "create_index(\n",
    "    elastic_client,\n",
    "    aoai_embeddings_index_name,\n",
    "    **mapping_properties\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7882d2ac",
   "metadata": {},
   "source": [
    "Next, we will calculate embeddings for our dataset.  Note that this step may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9091deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = datasets.get_dataframe_with_aoai_embeddings(\"msmarco\", embedding_model_deployment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b2b5e2",
   "metadata": {},
   "source": [
    "Then, we will index the data with embeddings we just calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_data(\n",
    "    elastic_client,\n",
    "    dataframe,\n",
    "    aoai_embeddings_index_name,\n",
    "    add_aoai_embeddings=True,\n",
    "    aoai_embedding_model_deployment=embedding_model_deployment_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c505efc9",
   "metadata": {},
   "source": [
    "Now that our index has been created with Azure Open AI embeddings, we can use the new index to chat with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea9580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_query = \"What's the weather like in Jamaica?\"\n",
    "chat_with_my_data(\n",
    "    chat_query,\n",
    "    chat_model_deployment_name,\n",
    "    elasticsearch_endpoint,\n",
    "    elasticsearch_api_key,\n",
    "    aoai_embeddings_index_name,\n",
    "    aoai_embedding_model=embedding_model_deployment_name,\n",
    "    aoai_embedding_key=openai.api_key\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
